Object.defineProperty(exports, '__esModule', { value: true });

var core = require('@urql/core');
var graphql_web = require('@0no-co/graphql.web');
var wonka = require('wonka');

// These are guards that are used throughout the codebase to warn or error on
// URL unfurls to https://formidable.com/open-source/urql/docs/graphcache/errors/
var helpUrl = '\nhttps://bit.ly/2XbVrpR#';
var cache = new Set();
var currentDebugStack = [];
var popDebugNode = () => currentDebugStack.pop();
var pushDebugNode = (typename, node) => {
  var identifier = '';
  if (node.kind === graphql_web.Kind.INLINE_FRAGMENT) {
    identifier = typename ? `Inline Fragment on "${typename}"` : 'Inline Fragment';
  } else if (node.kind === graphql_web.Kind.OPERATION_DEFINITION) {
    var name = node.name ? `"${node.name.value}"` : 'Unnamed';
    identifier = `${name} ${node.operation}`;
  } else if (node.kind === graphql_web.Kind.FRAGMENT_DEFINITION) {
    identifier = `"${node.name.value}" Fragment`;
  }
  if (identifier) {
    currentDebugStack.push(identifier);
  }
};
var getDebugOutput = () => currentDebugStack.length ? '\n(Caused At: ' + currentDebugStack.join(', ') + ')' : '';
function invariant(condition, message, code) {
  if (!condition) {
    var errorMessage = message || 'Minfied Error #' + code + '\n';
    if (process.env.NODE_ENV !== 'production') {
      errorMessage += getDebugOutput();
    }
    var error = new Error(errorMessage + helpUrl + code);
    error.name = 'Graphcache Error';
    throw error;
  }
}
function warn(message, code, logger) {
  if (!cache.has(message)) {
    if (logger) {
      logger('warn', message + getDebugOutput() + helpUrl + code);
    } else {
      console.warn(message + getDebugOutput() + helpUrl + code);
    }
    cache.add(message);
  }
}

var EMPTY_DIRECTIVES = {};

/** Returns the directives dictionary of a given node */
var getDirectives = node => node._directives || EMPTY_DIRECTIVES;

/** Returns the name of a given node */
var getName = node => node.name.value;
var getFragmentTypeName = node => node.typeCondition.name.value;

/** Returns either the field's name or the field's alias */
var getFieldAlias = node => node.alias ? node.alias.value : node.name.value;
var emptySelectionSet = [];

/** Returns the SelectionSet for a given inline or defined fragment node */
var getSelectionSet = node => node.selectionSet ? node.selectionSet.selections : emptySelectionSet;
var getTypeCondition = node => node.typeCondition ? node.typeCondition.name.value : null;

/** Evaluates a fields arguments taking vars into account */
var getFieldArguments = (node, vars) => {
  var args = null;
  if (node.arguments) {
    for (var i = 0, l = node.arguments.length; i < l; i++) {
      var arg = node.arguments[i];
      var value = graphql_web.valueFromASTUntyped(arg.value, vars);
      if (value !== undefined && value !== null) {
        if (!args) args = {};
        args[getName(arg)] = value;
      }
    }
  }
  return args;
};

/** Returns a filtered form of variables with values missing that the query doesn't require */
var filterVariables = (node, input) => {
  if (!input || !node.variableDefinitions) {
    return undefined;
  }
  var vars = {};
  for (var i = 0, l = node.variableDefinitions.length; i < l; i++) {
    var name = getName(node.variableDefinitions[i].variable);
    vars[name] = input[name];
  }
  return vars;
};

/** Returns a normalized form of variables with defaulted values */
var normalizeVariables = (node, input) => {
  var vars = {};
  if (!input) return vars;
  if (node.variableDefinitions) {
    for (var i = 0, l = node.variableDefinitions.length; i < l; i++) {
      var def = node.variableDefinitions[i];
      var name = getName(def.variable);
      vars[name] = input[name] === undefined && def.defaultValue ? graphql_web.valueFromASTUntyped(def.defaultValue, input) : input[name];
    }
  }
  for (var key in input) {
    if (!(key in vars)) vars[key] = input[key];
  }
  return vars;
};

/** Returns the main operation's definition */
function getMainOperation(doc) {
  for (var i = 0; i < doc.definitions.length; i++) {
    if (doc.definitions[i].kind === graphql_web.Kind.OPERATION_DEFINITION) {
      return doc.definitions[i];
    }
  }
  invariant(false, process.env.NODE_ENV !== "production" ? 'Invalid GraphQL document: All GraphQL documents must contain an OperationDefinition' + 'node for a query, subscription, or mutation.' : "", 1);
}

/** Returns a mapping from fragment names to their selections */
var getFragments = doc => {
  var fragments = {};
  for (var i = 0; i < doc.definitions.length; i++) {
    var node = doc.definitions[i];
    if (node.kind === graphql_web.Kind.FRAGMENT_DEFINITION) {
      fragments[getName(node)] = node;
    }
  }
  return fragments;
};

/** Resolves @include and @skip directives to determine whether field is included. */
var shouldInclude = (node, vars) => {
  var directives = getDirectives(node);
  if (directives.include || directives.skip) {
    // Finds any @include or @skip directive that forces the node to be skipped
    for (var name in directives) {
      var directive = directives[name];
      if (directive && (name === 'include' || name === 'skip') && directive.arguments && directive.arguments[0] && getName(directive.arguments[0]) === 'if') {
        // Return whether this directive forces us to skip
        // `@include(if: false)` or `@skip(if: true)`
        var value = graphql_web.valueFromASTUntyped(directive.arguments[0].value, vars);
        return name === 'include' ? !!value : !value;
      }
    }
  }
  return true;
};

/** Resolves @defer directive to determine whether a fragment is potentially skipped. */
var isDeferred = (node, vars) => {
  var {
    defer
  } = getDirectives(node);
  if (defer) {
    for (var argument of defer.arguments || []) {
      if (getName(argument) === 'if') {
        // Return whether `@defer(if: )` is enabled
        return !!graphql_web.valueFromASTUntyped(argument.value, vars);
      }
    }
    return true;
  }
  return false;
};

/** Resolves @_optional and @_required directive to determine whether the fields in a fragment are conaidered optional. */
var isOptional = node => {
  var {
    optional,
    required
  } = getDirectives(node);
  if (required) {
    return false;
  }
  if (optional) {
    return true;
  }
  return undefined;
};

var buildClientSchema = ({
  __schema
}) => {
  var typemap = new Map();
  var buildNameMap = arr => {
    var map;
    return () => {
      if (!map) {
        map = {};
        for (var i = 0; i < arr.length; i++) map[arr[i].name] = arr[i];
      }
      return map;
    };
  };
  var buildType = type => {
    switch (type.kind) {
      case 'OBJECT':
      case 'INTERFACE':
        return {
          name: type.name,
          kind: type.kind,
          interfaces: buildNameMap(type.interfaces || []),
          fields: buildNameMap(type.fields.map(field => ({
            name: field.name,
            type: field.type,
            args: buildNameMap(field.args)
          })))
        };
      case 'UNION':
        return {
          name: type.name,
          kind: type.kind,
          types: buildNameMap(type.possibleTypes || [])
        };
    }
  };
  var schema = {
    query: __schema.queryType ? __schema.queryType.name : null,
    mutation: __schema.mutationType ? __schema.mutationType.name : null,
    subscription: __schema.subscriptionType ? __schema.subscriptionType.name : null,
    types: undefined,
    isSubType(abstract, possible) {
      var abstractType = typemap.get(abstract);
      var possibleType = typemap.get(possible);
      if (!abstractType || !possibleType) {
        return false;
      } else if (abstractType.kind === 'UNION') {
        return !!abstractType.types()[possible];
      } else if (abstractType.kind !== 'OBJECT' && possibleType.kind === 'OBJECT') {
        return !!possibleType.interfaces()[abstract];
      } else {
        return abstract === possible;
      }
    }
  };
  if (__schema.types) {
    schema.types = typemap;
    for (var i = 0; i < __schema.types.length; i++) {
      var type = __schema.types[i];
      if (type && type.name) {
        var out = buildType(type);
        if (out) typemap.set(type.name, out);
      }
    }
  }
  return schema;
};

var BUILTIN_NAME = '__';
var isFieldNullable = (schema, typename, fieldName, logger) => {
  var field = getField(schema, typename, fieldName, logger);
  return !!field && field.type.kind !== 'NON_NULL';
};
var isListNullable = (schema, typename, fieldName, logger) => {
  var field = getField(schema, typename, fieldName, logger);
  if (!field) return false;
  var ofType = field.type.kind === 'NON_NULL' ? field.type.ofType : field.type;
  return ofType.kind === 'LIST' && ofType.ofType.kind !== 'NON_NULL';
};
var isFieldAvailableOnType = (schema, typename, fieldName, logger) => fieldName.indexOf(BUILTIN_NAME) === 0 || typename.indexOf(BUILTIN_NAME) === 0 || !!getField(schema, typename, fieldName, logger);
var isInterfaceOfType = (schema, node, typename) => {
  if (!typename) return false;
  var typeCondition = getTypeCondition(node);
  if (!typeCondition || typename === typeCondition) {
    return true;
  } else if (schema.types.has(typeCondition) && schema.types.get(typeCondition).kind === 'OBJECT') {
    return typeCondition === typename;
  }
  expectAbstractType(schema, typeCondition);
  expectObjectType(schema, typename);
  return schema.isSubType(typeCondition, typename);
};
var getField = (schema, typename, fieldName, logger) => {
  if (fieldName.indexOf(BUILTIN_NAME) === 0 || typename.indexOf(BUILTIN_NAME) === 0) return;
  expectObjectType(schema, typename);
  var object = schema.types.get(typename);
  var field = object.fields()[fieldName];
  if (process.env.NODE_ENV !== 'production') {
    if (!field) {
      warn('Invalid field: The field `' + fieldName + '` does not exist on `' + typename + '`, ' + 'but the GraphQL document expects it to exist.\n' + 'Traversal will continue, however this may lead to undefined behavior!', 4, logger);
    }
  }
  return field;
};
function expectObjectType(schema, typename) {
  invariant(schema.types.has(typename) && schema.types.get(typename).kind === 'OBJECT', process.env.NODE_ENV !== "production" ? 'Invalid Object type: The type `' + typename + '` is not an object in the defined schema, ' + 'but the GraphQL document is traversing it.' : "", 3);
}
function expectAbstractType(schema, typename) {
  invariant(schema.types.has(typename) && (schema.types.get(typename).kind === 'INTERFACE' || schema.types.get(typename).kind === 'UNION'), process.env.NODE_ENV !== "production" ? 'Invalid Abstract type: The type `' + typename + '` is not an Interface or Union type in the defined schema, ' + 'but a fragment in the GraphQL document is using it as a type condition.' : "", 5);
}
function expectValidKeyingConfig(schema, keys, logger) {
  if (process.env.NODE_ENV !== 'production') {
    for (var key in keys) {
      if (process.env.NODE_ENV !== 'production') {
        if (!schema.types.has(key)) {
          warn('Invalid Object type: The type `' + key + '` is not an object in the defined schema, but the `keys` option is referencing it.', 20, logger);
        }
      }
    }
  }
}
function expectValidUpdatesConfig(schema, updates, logger) {
  if (process.env.NODE_ENV === 'production') {
    return;
  }
  for (var typename in updates) {
    if (!updates[typename]) {
      continue;
    } else if (!schema.types.has(typename)) {
      var addition = '';
      if (typename === 'Mutation' && schema.mutation && schema.mutation !== 'Mutation') {
        addition += '\nMaybe your config should reference `' + schema.mutation + '`?';
      } else if (typename === 'Subscription' && schema.subscription && schema.subscription !== 'Subscription') {
        addition += '\nMaybe your config should reference `' + schema.subscription + '`?';
      }
      return process.env.NODE_ENV !== 'production' ? warn('Invalid updates type: The type `' + typename + '` is not an object in the defined schema, but the `updates` config is referencing it.' + addition, 21, logger) : void 0;
    }
    var fields = schema.types.get(typename).fields();
    for (var fieldName in updates[typename]) {
      if (process.env.NODE_ENV !== 'production') {
        if (!fields[fieldName]) {
          warn('Invalid updates field: `' + fieldName + '` on `' + typename + '` is not in the defined schema, but the `updates` config is referencing it.', 22, logger);
        }
      }
    }
  }
}
function warnAboutResolver(name, logger) {
  process.env.NODE_ENV !== 'production' ? warn(`Invalid resolver: \`${name}\` is not in the defined schema, but the \`resolvers\` option is referencing it.`, 23, logger) : void 0;
}
function warnAboutAbstractResolver(name, kind, logger) {
  process.env.NODE_ENV !== 'production' ? warn(`Invalid resolver: \`${name}\` does not match to a concrete type in the schema, but the \`resolvers\` option is referencing it. Implement the resolver for the types that ${kind === 'UNION' ? 'make up the union' : 'implement the interface'} instead.`, 26, logger) : void 0;
}
function expectValidResolversConfig(schema, resolvers, logger) {
  if (process.env.NODE_ENV === 'production') {
    return;
  }
  for (var key in resolvers) {
    if (key === 'Query') {
      if (schema.query) {
        var validQueries = schema.types.get(schema.query).fields();
        for (var resolverQuery in resolvers.Query || {}) {
          if (!validQueries[resolverQuery]) {
            warnAboutResolver('Query.' + resolverQuery, logger);
          }
        }
      } else {
        warnAboutResolver('Query', logger);
      }
    } else {
      if (!schema.types.has(key)) {
        warnAboutResolver(key, logger);
      } else if (schema.types.get(key).kind === 'INTERFACE' || schema.types.get(key).kind === 'UNION') {
        warnAboutAbstractResolver(key, schema.types.get(key).kind, logger);
      } else {
        var validTypeProperties = schema.types.get(key).fields();
        for (var resolverProperty in resolvers[key] || {}) {
          if (!validTypeProperties[resolverProperty]) {
            warnAboutResolver(key + '.' + resolverProperty, logger);
          }
        }
      }
    }
  }
}
function expectValidOptimisticMutationsConfig(schema, optimisticMutations, logger) {
  if (process.env.NODE_ENV === 'production') {
    return;
  }
  if (schema.mutation) {
    var validMutations = schema.types.get(schema.mutation).fields();
    for (var mutation in optimisticMutations) {
      if (process.env.NODE_ENV !== 'production') {
        if (!validMutations[mutation]) {
          warn(`Invalid optimistic mutation field: \`${mutation}\` is not a mutation field in the defined schema, but the \`optimistic\` option is referencing it.`, 24, logger);
        }
      }
    }
  }
}

var keyOfField = (fieldName, args) => args ? `${fieldName}(${core.stringifyVariables(args)})` : fieldName;
var joinKeys = (parentKey, key) => `${parentKey}.${key}`;
var fieldInfoOfKey = fieldKey => {
  var parenIndex = fieldKey.indexOf('(');
  if (parenIndex > -1) {
    return {
      fieldKey,
      fieldName: fieldKey.slice(0, parenIndex),
      arguments: JSON.parse(fieldKey.slice(parenIndex + 1, -1))
    };
  } else {
    return {
      fieldKey,
      fieldName: fieldKey,
      arguments: null
    };
  }
};
var serializeKeys = (entityKey, fieldKey) => `${entityKey.replace(/\./g, '%2e')}.${fieldKey}`;
var deserializeKeyInfo = key => {
  var dotIndex = key.indexOf('.');
  var entityKey = key.slice(0, dotIndex).replace(/%2e/g, '.');
  var fieldKey = key.slice(dotIndex + 1);
  return {
    entityKey,
    fieldKey
  };
};

var currentOwnership = null;
var currentDataMapping = null;
var currentData = null;
var currentOptimisticKey = null;
var currentOperation = null;
var currentDependencies = null;
var currentForeignData = false;
var currentOptimistic = false;
/** Creates a new data object unless it's been created in this data run */
function makeData(data, isArray) {
  var newData;
  if (data) {
    if (currentOwnership.has(data)) return data;
    newData = currentDataMapping.get(data);
  }
  if (newData == null) {
    newData = isArray ? [] : {};
  }
  if (data) {
    currentDataMapping.set(data, newData);
  }
  currentOwnership.add(newData);
  return newData;
}
var ownsData = data => !!data && currentOwnership.has(data);

/** Before reading or writing the global state needs to be initialised */
var initDataState = (operationType, data, layerKey, isOptimistic, isForeignData) => {
  currentOwnership = new WeakSet();
  currentDataMapping = new WeakMap();
  currentOperation = operationType;
  currentData = data;
  currentDependencies = new Set();
  currentOptimistic = !!isOptimistic;
  currentForeignData = !!isForeignData;
  if (process.env.NODE_ENV !== 'production') {
    currentDebugStack.length = 0;
  }
  if (!layerKey) {
    currentOptimisticKey = null;
  } else if (currentOperation === 'read') {
    // We don't create new layers for read operations and instead simply
    // apply the currently available layer, if any
    currentOptimisticKey = layerKey;
  } else if (isOptimistic || data.hydrating || data.optimisticOrder.length > 1) {
    // If this operation isn't optimistic and we see it for the first time,
    // then it must've been optimistic in the past, so we can proactively
    // clear the optimistic data before writing
    if (!isOptimistic && !data.commutativeKeys.has(layerKey)) {
      reserveLayer(data, layerKey);
    } else if (isOptimistic) {
      if (data.optimisticOrder.indexOf(layerKey) !== -1 && !data.commutativeKeys.has(layerKey)) {
        data.optimisticOrder.splice(data.optimisticOrder.indexOf(layerKey), 1);
      }
      // NOTE: This optimally shouldn't happen as it implies that an optimistic
      // write is being performed after a concrete write.
      data.commutativeKeys.delete(layerKey);
    }

    // An optimistic update of a mutation may force an optimistic layer,
    // or this Query update may be applied optimistically since it's part
    // of a commutative chain
    currentOptimisticKey = layerKey;
    createLayer(data, layerKey);
  } else {
    // Otherwise we don't create an optimistic layer and clear the
    // operation's one if it already exists
    // We also do this when only one layer exists to avoid having to squash
    // any layers at the end of writing this layer
    currentOptimisticKey = null;
    deleteLayer(data, layerKey);
  }
};

/** Reset the data state after read/write is complete */
var clearDataState = () => {
  // NOTE: This is only called to check for the invariant to pass
  if (process.env.NODE_ENV !== 'production') {
    getCurrentDependencies();
  }
  var data = currentData;
  var layerKey = currentOptimisticKey;
  currentOptimistic = false;
  currentOptimisticKey = null;

  // Determine whether the current operation has been a commutative layer
  if (!data.hydrating && layerKey && data.optimisticOrder.indexOf(layerKey) > -1) {
    // Squash all layers in reverse order (low priority upwards) that have
    // been written already
    var i = data.optimisticOrder.length;
    while (--i >= 0 && data.dirtyKeys.has(data.optimisticOrder[i]) && data.commutativeKeys.has(data.optimisticOrder[i])) squashLayer(data.optimisticOrder[i]);
  }
  currentOwnership = null;
  currentDataMapping = null;
  currentOperation = null;
  currentData = null;
  currentDependencies = null;
  if (process.env.NODE_ENV !== 'production') {
    currentDebugStack.length = 0;
  }
  if (process.env.NODE_ENV !== 'test') {
    // Schedule deferred tasks if we haven't already, and if either a persist or GC run
    // are likely to be needed
    if (!data.defer && (data.storage || !data.optimisticOrder.length)) {
      data.defer = true;
      setTimeout(() => {
        initDataState('read', data, null);
        gc();
        persistData();
        clearDataState();
        data.defer = false;
      });
    }
  }
};

/** Initialises then resets the data state, which may squash this layer if necessary */
var noopDataState = (data, layerKey, isOptimistic) => {
  if (layerKey && !isOptimistic) data.deferredKeys.delete(layerKey);
  initDataState('write', data, layerKey, isOptimistic);
  clearDataState();
};

/** As we're writing, we keep around all the records and links we've read or have written to */
var getCurrentDependencies = () => {
  invariant(currentDependencies !== null, process.env.NODE_ENV !== "production" ? 'Invalid Cache call: The cache may only be accessed or mutated during' + 'operations like write or query, or as part of its resolvers, updaters, ' + 'or optimistic configs.' : "", 2);
  return currentDependencies;
};
var DEFAULT_EMPTY_SET = new Set();
var make = queryRootKey => ({
  hydrating: false,
  defer: false,
  gc: new Set(),
  types: new Map(),
  persist: new Set(),
  queryRootKey,
  refCount: new Map(),
  links: {
    optimistic: new Map(),
    base: new Map()
  },
  records: {
    optimistic: new Map(),
    base: new Map()
  },
  deferredKeys: new Set(),
  commutativeKeys: new Set(),
  dirtyKeys: new Set(),
  optimisticOrder: [],
  storage: null
});

/** Adds a node value to a NodeMap (taking optimistic values into account */
var setNode = (map, entityKey, fieldKey, value) => {
  if (process.env.NODE_ENV !== 'production') {
    invariant(currentOperation !== 'read', process.env.NODE_ENV !== "production" ? 'Invalid Cache write: You may not write to the cache during cache reads. ' + ' Accesses to `cache.writeFragment`, `cache.updateQuery`, and `cache.link` may ' + ' not be made inside `resolvers` for instance.' : "", 27);
  }

  // Optimistic values are written to a map in the optimistic dict
  // All other values are written to the base map
  var keymap = currentOptimisticKey ? map.optimistic.get(currentOptimisticKey) : map.base;

  // On the map itself we get or create the entity as a dict
  var entity = keymap.get(entityKey);
  if (entity === undefined) {
    keymap.set(entityKey, entity = Object.create(null));
  }

  // If we're setting undefined we delete the node's entry
  // On optimistic layers we actually set undefined so it can
  // override the base value
  if (value === undefined && !currentOptimisticKey) {
    delete entity[fieldKey];
  } else {
    entity[fieldKey] = value;
  }
};

/** Gets a node value from a NodeMap (taking optimistic values into account */
var getNode = (map, entityKey, fieldKey) => {
  var node;
  // A read may be initialised to skip layers until its own, which is useful for
  // reading back written data. It won't skip over optimistic layers however
  var skip = !currentOptimistic && currentOperation === 'read' && currentOptimisticKey && currentData.commutativeKeys.has(currentOptimisticKey);
  // This first iterates over optimistic layers (in order)
  for (var i = 0, l = currentData.optimisticOrder.length; i < l; i++) {
    var layerKey = currentData.optimisticOrder[i];
    var optimistic = map.optimistic.get(layerKey);
    // If we're reading starting from a specific layer, we skip until a match
    skip = skip && layerKey !== currentOptimisticKey;
    // If the node and node value exists it is returned, including undefined
    if (optimistic && (!skip || !currentData.commutativeKeys.has(layerKey)) && (!currentOptimistic || currentOperation === 'write' || currentData.commutativeKeys.has(layerKey)) && (node = optimistic.get(entityKey)) !== undefined && fieldKey in node) {
      return node[fieldKey];
    }
  }

  // Otherwise we read the non-optimistic base value
  node = map.base.get(entityKey);
  return node !== undefined ? node[fieldKey] : undefined;
};

/** Adjusts the reference count of an entity on a refCount dict by "by" and updates the gc */
var updateRCForEntity = (entityKey, by) => {
  // Retrieve the reference count and adjust it by "by"
  var count = currentData.refCount.get(entityKey) || 0;
  var newCount = count + by > 0 ? count + by : 0;
  currentData.refCount.set(entityKey, newCount);
  // Add it to the garbage collection batch if it needs to be deleted or remove it
  // from the batch if it needs to be kept
  if (!newCount) currentData.gc.add(entityKey);else if (!count && newCount) currentData.gc.delete(entityKey);
};

/** Adjusts the reference counts of all entities of a link on a refCount dict by "by" and updates the gc */
var updateRCForLink = (link, by) => {
  if (Array.isArray(link)) {
    for (var i = 0, l = link.length; i < l; i++) updateRCForLink(link[i], by);
  } else if (typeof link === 'string') {
    updateRCForEntity(link, by);
  }
};

/** Writes all parsed FieldInfo objects of a given node dict to a given array if it hasn't been seen */
var extractNodeFields = (fieldInfos, seenFieldKeys, node) => {
  if (node !== undefined) {
    for (var fieldKey in node) {
      if (!seenFieldKeys.has(fieldKey)) {
        // If the node hasn't been seen the serialized fieldKey is turnt back into
        // a rich FieldInfo object that also contains the field's name and arguments
        fieldInfos.push(fieldInfoOfKey(fieldKey));
        seenFieldKeys.add(fieldKey);
      }
    }
  }
};

/** Writes all parsed FieldInfo objects of all nodes in a NodeMap to a given array */
var extractNodeMapFields = (fieldInfos, seenFieldKeys, entityKey, map) => {
  // Extracts FieldInfo for the entity in the base map
  extractNodeFields(fieldInfos, seenFieldKeys, map.base.get(entityKey));

  // Then extracts FieldInfo for the entity from the optimistic maps
  for (var i = 0, l = currentData.optimisticOrder.length; i < l; i++) {
    var optimistic = map.optimistic.get(currentData.optimisticOrder[i]);
    if (optimistic !== undefined) {
      extractNodeFields(fieldInfos, seenFieldKeys, optimistic.get(entityKey));
    }
  }
};

/** Garbage collects all entities that have been marked as having no references */
var gc = () => {
  // If we're currently awaiting deferred results, abort GC run
  if (currentData.optimisticOrder.length) return;

  // Iterate over all entities that have been marked for deletion
  // Entities have been marked for deletion in `updateRCForEntity` if
  // their reference count dropped to 0
  for (var entityKey of currentData.gc.keys()) {
    // Remove the current key from the GC batch
    currentData.gc.delete(entityKey);

    // Check first whether the entity has any references,
    // if so, we skip it from the GC run
    var rc = currentData.refCount.get(entityKey) || 0;
    if (rc > 0) continue;
    var record = currentData.records.base.get(entityKey);
    // Delete the reference count, and delete the entity from the GC batch
    currentData.refCount.delete(entityKey);
    currentData.records.base.delete(entityKey);
    var typename = record && record.__typename;
    if (typename) {
      var type = currentData.types.get(typename);
      if (type) type.delete(entityKey);
    }
    var linkNode = currentData.links.base.get(entityKey);
    if (linkNode) {
      currentData.links.base.delete(entityKey);
      for (var fieldKey in linkNode) updateRCForLink(linkNode[fieldKey], -1);
    }
  }
};
var updateDependencies = (entityKey, fieldKey) => {
  if (entityKey !== currentData.queryRootKey) {
    currentDependencies.add(entityKey);
  } else if (fieldKey !== undefined && fieldKey !== '__typename') {
    currentDependencies.add(joinKeys(entityKey, fieldKey));
  }
};
var updatePersist = (entityKey, fieldKey) => {
  if (!currentOptimistic && currentData.storage) {
    currentData.persist.add(serializeKeys(entityKey, fieldKey));
  }
};

/** Reads an entity's field (a "record") from data */
var readRecord = (entityKey, fieldKey) => {
  updateDependencies(entityKey, fieldKey);
  return getNode(currentData.records, entityKey, fieldKey);
};

/** Reads an entity's link from data */
var readLink = (entityKey, fieldKey) => {
  updateDependencies(entityKey, fieldKey);
  return getNode(currentData.links, entityKey, fieldKey);
};
var getEntitiesForType = typename => currentData.types.get(typename) || DEFAULT_EMPTY_SET;
var writeType = (typename, entityKey) => {
  var existingTypes = currentData.types.get(typename);
  if (!existingTypes) {
    var typeSet = new Set();
    typeSet.add(entityKey);
    currentData.types.set(typename, typeSet);
  } else {
    existingTypes.add(entityKey);
  }
};

/** Writes an entity's field (a "record") to data */
var writeRecord = (entityKey, fieldKey, value) => {
  updateDependencies(entityKey, fieldKey);
  updatePersist(entityKey, fieldKey);
  setNode(currentData.records, entityKey, fieldKey, value);
};
var hasField = (entityKey, fieldKey) => readRecord(entityKey, fieldKey) !== undefined || readLink(entityKey, fieldKey) !== undefined;

/** Writes an entity's link to data */
var writeLink = (entityKey, fieldKey, link) => {
  // Retrieve the link NodeMap from either an optimistic or the base layer
  var links = currentOptimisticKey ? currentData.links.optimistic.get(currentOptimisticKey) : currentData.links.base;
  // Update the reference count for the link
  if (!currentOptimisticKey) {
    var entityLinks = links && links.get(entityKey);
    updateRCForLink(entityLinks && entityLinks[fieldKey], -1);
    updateRCForLink(link, 1);
  }
  // Update persistence batch and dependencies
  updateDependencies(entityKey, fieldKey);
  updatePersist(entityKey, fieldKey);
  // Update the link
  setNode(currentData.links, entityKey, fieldKey, link);
};

/** Reserves an optimistic layer and preorders it */
var reserveLayer = (data, layerKey, hasNext) => {
  // Find the current index for the layer, and remove it from
  // the order if it exists already
  var index = data.optimisticOrder.indexOf(layerKey);
  if (index > -1) data.optimisticOrder.splice(index, 1);
  if (hasNext) {
    data.deferredKeys.add(layerKey);
    // If the layer has future results then we'll move it past any layer that's
    // still empty, so currently pending operations will take precedence over it
    for (index = index > -1 ? index : 0; index < data.optimisticOrder.length && !data.deferredKeys.has(data.optimisticOrder[index]) && (!data.dirtyKeys.has(data.optimisticOrder[index]) || !data.commutativeKeys.has(data.optimisticOrder[index])); index++);
  } else {
    data.deferredKeys.delete(layerKey);
    // Protect optimistic layers from being turned into non-optimistic layers
    // while preserving optimistic data
    if (index > -1 && !data.commutativeKeys.has(layerKey)) clearLayer(data, layerKey);
    index = 0;
  }

  // Register the layer with the deferred or "top" index and
  // mark it as commutative
  data.optimisticOrder.splice(index, 0, layerKey);
  data.commutativeKeys.add(layerKey);
};

/** Checks whether a given layer exists */
var hasLayer = (data, layerKey) => data.commutativeKeys.has(layerKey) || data.optimisticOrder.indexOf(layerKey) > -1;

/** Creates an optimistic layer of links and records */
var createLayer = (data, layerKey) => {
  if (data.optimisticOrder.indexOf(layerKey) === -1) {
    data.optimisticOrder.unshift(layerKey);
  }
  if (!data.dirtyKeys.has(layerKey)) {
    data.dirtyKeys.add(layerKey);
    data.links.optimistic.set(layerKey, new Map());
    data.records.optimistic.set(layerKey, new Map());
  }
};

/** Clears all links and records of an optimistic layer */
var clearLayer = (data, layerKey) => {
  if (data.dirtyKeys.has(layerKey)) {
    data.dirtyKeys.delete(layerKey);
    data.records.optimistic.delete(layerKey);
    data.links.optimistic.delete(layerKey);
    data.deferredKeys.delete(layerKey);
  }
};

/** Deletes links and records of an optimistic layer, and the layer itself */
var deleteLayer = (data, layerKey) => {
  var index = data.optimisticOrder.indexOf(layerKey);
  if (index > -1) {
    data.optimisticOrder.splice(index, 1);
    data.commutativeKeys.delete(layerKey);
  }
  clearLayer(data, layerKey);
};

/** Merges an optimistic layer of links and records into the base data */
var squashLayer = layerKey => {
  // Hide current dependencies from squashing operations
  var previousDependencies = currentDependencies;
  currentDependencies = new Set();
  currentOperation = 'write';
  var links = currentData.links.optimistic.get(layerKey);
  if (links) {
    for (var entry of links.entries()) {
      var entityKey = entry[0];
      var keyMap = entry[1];
      for (var fieldKey in keyMap) writeLink(entityKey, fieldKey, keyMap[fieldKey]);
    }
  }
  var records = currentData.records.optimistic.get(layerKey);
  if (records) {
    for (var _entry of records.entries()) {
      var _entityKey = _entry[0];
      var _keyMap = _entry[1];
      for (var _fieldKey in _keyMap) writeRecord(_entityKey, _fieldKey, _keyMap[_fieldKey]);
    }
  }
  currentDependencies = previousDependencies;
  deleteLayer(currentData, layerKey);
};

/** Return an array of FieldInfo (info on all the fields and their arguments) for a given entity */
var inspectFields = entityKey => {
  var {
    links,
    records
  } = currentData;
  var fieldInfos = [];
  var seenFieldKeys = new Set();
  // Update dependencies
  updateDependencies(entityKey);
  // Extract FieldInfos to the fieldInfos array for links and records
  // This also deduplicates by keeping track of fieldKeys in the seenFieldKeys Set
  extractNodeMapFields(fieldInfos, seenFieldKeys, entityKey, links);
  extractNodeMapFields(fieldInfos, seenFieldKeys, entityKey, records);
  return fieldInfos;
};
var persistData = () => {
  if (currentData.storage) {
    currentOptimistic = true;
    currentOperation = 'read';
    var entries = {};
    for (var key of currentData.persist.keys()) {
      var {
        entityKey,
        fieldKey
      } = deserializeKeyInfo(key);
      var x = void 0;
      if ((x = readLink(entityKey, fieldKey)) !== undefined) {
        entries[key] = `:${core.stringifyVariables(x)}`;
      } else if ((x = readRecord(entityKey, fieldKey)) !== undefined) {
        entries[key] = core.stringifyVariables(x);
      } else {
        entries[key] = undefined;
      }
    }
    currentOptimistic = false;
    currentData.storage.writeData(entries);
    currentData.persist.clear();
  }
};
var hydrateData = (data, storage, entries) => {
  initDataState('write', data, null);
  for (var key in entries) {
    var value = entries[key];
    if (value !== undefined) {
      var {
        entityKey,
        fieldKey
      } = deserializeKeyInfo(key);
      if (value[0] === ':') {
        if (readLink(entityKey, fieldKey) === undefined) writeLink(entityKey, fieldKey, JSON.parse(value.slice(1)));
      } else {
        if (readRecord(entityKey, fieldKey) === undefined) writeRecord(entityKey, fieldKey, JSON.parse(value));
      }
    }
  }
  data.storage = storage;
  data.hydrating = false;
  clearDataState();
};

var contextRef = null;
var deferRef = false;
var optionalRef = undefined;

// Checks whether the current data field is a cache miss because of a GraphQLError
var getFieldError = ctx => ctx.__internal.path.length > 0 && ctx.__internal.errorMap ? ctx.__internal.errorMap[ctx.__internal.path.join('.')] : undefined;
var makeContext = (store, variables, fragments, typename, entityKey, error) => {
  var ctx = {
    store,
    variables,
    fragments,
    parent: {
      __typename: typename
    },
    parentTypeName: typename,
    parentKey: entityKey,
    parentFieldKey: '',
    fieldName: '',
    error: undefined,
    partial: false,
    hasNext: false,
    optimistic: currentOptimistic,
    __internal: {
      path: [],
      errorMap: undefined
    }
  };
  if (error && error.graphQLErrors) {
    for (var i = 0; i < error.graphQLErrors.length; i++) {
      var graphQLError = error.graphQLErrors[i];
      if (graphQLError.path && graphQLError.path.length) {
        if (!ctx.__internal.errorMap) ctx.__internal.errorMap = Object.create(null);
        ctx.__internal.errorMap[graphQLError.path.join('.')] = graphQLError;
      }
    }
  }
  return ctx;
};
var updateContext = (ctx, data, typename, entityKey, fieldKey, fieldName) => {
  contextRef = ctx;
  ctx.parent = data;
  ctx.parentTypeName = typename;
  ctx.parentKey = entityKey;
  ctx.parentFieldKey = fieldKey;
  ctx.fieldName = fieldName;
  ctx.error = getFieldError(ctx);
};
var isFragmentHeuristicallyMatching = (node, typename, entityKey, vars, logger) => {
  if (!typename) return false;
  var typeCondition = getTypeCondition(node);
  if (!typeCondition || typename === typeCondition) return true;
  process.env.NODE_ENV !== 'production' ? warn('Heuristic Fragment Matching: A fragment is trying to match against the `' + typename + '` type, ' + 'but the type condition is `' + typeCondition + '`. Since GraphQL allows for interfaces `' + typeCondition + '` may be an ' + 'interface.\nA schema needs to be defined for this match to be deterministic, ' + 'otherwise the fragment will be matched heuristically!', 16, logger) : void 0;
  return currentOperation === 'write' || !getSelectionSet(node).some(node => {
    if (node.kind !== graphql_web.Kind.FIELD) return false;
    var fieldKey = keyOfField(getName(node), getFieldArguments(node, vars));
    return !hasField(entityKey, fieldKey);
  });
};

// NOTE: Outside of this file, we expect `_defer` to always be reset to `false`

// NOTE: Inside this file we expect the state to be recursively passed on

function makeSelectionIterator(typename, entityKey, _defer, _optional, selectionSet, ctx) {
  var child;
  var index = 0;
  return function next() {
    var node;
    while (child || index < selectionSet.length) {
      node = undefined;
      deferRef = _defer;
      optionalRef = _optional;
      if (child) {
        if (node = child()) {
          return node;
        } else {
          child = undefined;
          if (process.env.NODE_ENV !== 'production') popDebugNode();
        }
      } else {
        var select = selectionSet[index++];
        if (!shouldInclude(select, ctx.variables)) ; else if (select.kind !== graphql_web.Kind.FIELD) {
          // A fragment is either referred to by FragmentSpread or inline
          var fragment = select.kind !== graphql_web.Kind.INLINE_FRAGMENT ? ctx.fragments[getName(select)] : select;
          if (fragment) {
            var isMatching = !fragment.typeCondition || (ctx.store.schema ? isInterfaceOfType(ctx.store.schema, fragment, typename) : isFragmentHeuristicallyMatching(fragment, typename, entityKey, ctx.variables, ctx.store.logger));
            if (isMatching) {
              if (process.env.NODE_ENV !== 'production') pushDebugNode(typename, fragment);
              var isFragmentOptional = isOptional(select);
              child = makeSelectionIterator(typename, entityKey, _defer || isDeferred(select, ctx.variables), isFragmentOptional !== undefined ? isFragmentOptional : _optional, getSelectionSet(fragment), ctx);
            }
          }
        } else if (currentOperation === 'write' || !select._generated) {
          return select;
        }
      }
    }
  };
}
var ensureData = x => x == null ? null : x;
var ensureLink = (store, ref) => {
  if (!ref) {
    return ref || null;
  } else if (Array.isArray(ref)) {
    var _link = new Array(ref.length);
    for (var i = 0, l = _link.length; i < l; i++) _link[i] = ensureLink(store, ref[i]);
    return _link;
  }
  var link = store.keyOfEntity(ref);
  if (process.env.NODE_ENV !== 'production') {
    if (!link && ref && typeof ref === 'object') {
      warn("Can't generate a key for link(...) item." + '\nYou have to pass an `id` or `_id` field or create a custom `keys` config for `' + ref.__typename + '`.', 12, store.logger);
    }
  }
  return link;
};

/** Reads a GraphQL query from the cache.
 * @internal
 */
var _query = (store, request, input, error) => {
  var query = core.formatDocument(request.query);
  var operation = getMainOperation(query);
  var rootKey = store.rootFields[operation.operation];
  var rootSelect = getSelectionSet(operation);
  var ctx = makeContext(store, normalizeVariables(operation, request.variables), getFragments(query), rootKey, rootKey, error);
  if (process.env.NODE_ENV !== 'production') {
    pushDebugNode(rootKey, operation);
  }

  // NOTE: This may reuse "previous result data" as indicated by the
  // `originalData` argument in readRoot(). This behaviour isn't used
  // for readSelection() however, which always produces results from
  // scratch
  var data = rootKey !== ctx.store.rootFields['query'] ? readRoot(ctx, rootKey, rootSelect, input || makeData()) : readSelection(ctx, rootKey, rootSelect, input || makeData());
  if (process.env.NODE_ENV !== 'production') {
    popDebugNode();
    getCurrentDependencies();
  }
  return {
    dependencies: currentDependencies,
    partial: ctx.partial || !data,
    hasNext: ctx.hasNext,
    data: data || null
  };
};
var readRoot = (ctx, entityKey, select, input) => {
  var typename = ctx.store.rootNames[entityKey] ? entityKey : input.__typename;
  if (typeof typename !== 'string') {
    return input;
  }
  var iterate = makeSelectionIterator(entityKey, entityKey, false, undefined, select, ctx);
  var node;
  var hasChanged = currentForeignData;
  var output = makeData(input);
  while (node = iterate()) {
    var fieldAlias = getFieldAlias(node);
    var fieldValue = input[fieldAlias];
    // Add the current alias to the walked path before processing the field's value
    ctx.__internal.path.push(fieldAlias);
    // We temporarily store the data field in here, but undefined
    // means that the value is missing from the cache
    var dataFieldValue = void 0;
    if (node.selectionSet && fieldValue !== null) {
      dataFieldValue = readRootField(ctx, getSelectionSet(node), ensureData(fieldValue));
    } else {
      dataFieldValue = fieldValue;
    }

    // Check for any referential changes in the field's value
    hasChanged = hasChanged || dataFieldValue !== fieldValue;
    if (dataFieldValue !== undefined) output[fieldAlias] = dataFieldValue;

    // After processing the field, remove the current alias from the path again
    ctx.__internal.path.pop();
  }
  return hasChanged ? output : input;
};
var readRootField = (ctx, select, originalData) => {
  if (Array.isArray(originalData)) {
    var newData = new Array(originalData.length);
    var hasChanged = currentForeignData;
    for (var i = 0, l = originalData.length; i < l; i++) {
      // Add the current index to the walked path before reading the field's value
      ctx.__internal.path.push(i);
      // Recursively read the root field's value
      newData[i] = readRootField(ctx, select, originalData[i]);
      hasChanged = hasChanged || newData[i] !== originalData[i];
      // After processing the field, remove the current index from the path
      ctx.__internal.path.pop();
    }
    return hasChanged ? newData : originalData;
  } else if (originalData === null) {
    return null;
  }

  // Write entity to key that falls back to the given parentFieldKey
  var entityKey = ctx.store.keyOfEntity(originalData);
  if (entityKey !== null) {
    // We assume that since this is used for result data this can never be undefined,
    // since the result data has already been written to the cache
    return readSelection(ctx, entityKey, select, originalData) || null;
  } else {
    return readRoot(ctx, originalData.__typename, select, originalData);
  }
};
var _queryFragment = (store, query, entity, variables, fragmentName) => {
  var fragments = getFragments(query);
  var fragment;
  if (fragmentName) {
    fragment = fragments[fragmentName];
    if (!fragment) {
      process.env.NODE_ENV !== 'production' ? warn('readFragment(...) was called with a fragment name that does not exist.\n' + 'You provided ' + fragmentName + ' but could only find ' + Object.keys(fragments).join(', ') + '.', 6, store.logger) : void 0;
      return null;
    }
  } else {
    var names = Object.keys(fragments);
    fragment = fragments[names[0]];
    if (!fragment) {
      process.env.NODE_ENV !== 'production' ? warn('readFragment(...) was called with an empty fragment.\n' + 'You have to call it with at least one fragment in your GraphQL document.', 6, store.logger) : void 0;
      return null;
    }
  }
  var typename = getFragmentTypeName(fragment);
  if (typeof entity !== 'string' && !entity.__typename) entity.__typename = typename;
  var entityKey = store.keyOfEntity(entity);
  if (!entityKey) {
    process.env.NODE_ENV !== 'production' ? warn("Can't generate a key for readFragment(...).\n" + 'You have to pass an `id` or `_id` field or create a custom `keys` config for `' + typename + '`.', 7, store.logger) : void 0;
    return null;
  }
  if (process.env.NODE_ENV !== 'production') {
    pushDebugNode(typename, fragment);
  }
  var ctx = makeContext(store, variables || {}, fragments, typename, entityKey, undefined);
  var result = readSelection(ctx, entityKey, getSelectionSet(fragment), makeData()) || null;
  if (process.env.NODE_ENV !== 'production') {
    popDebugNode();
  }
  return result;
};
function getFieldResolver(directives, typename, fieldName, ctx) {
  var resolvers = ctx.store.resolvers[typename];
  var fieldResolver = resolvers && resolvers[fieldName];
  var directiveResolver;
  for (var name in directives) {
    var directiveNode = directives[name];
    if (directiveNode && name !== 'include' && name !== 'skip' && ctx.store.directives[name]) {
      directiveResolver = ctx.store.directives[name](getFieldArguments(directiveNode, ctx.variables));
      if (process.env.NODE_ENV === 'production') return directiveResolver;
      break;
    }
  }
  if (process.env.NODE_ENV !== 'production') {
    if (fieldResolver && directiveResolver) {
      warn(`A resolver and directive is being used at "${typename}.${fieldName}" simultaneously. Only the directive will apply.`, 28, ctx.store.logger);
    }
  }
  return directiveResolver || fieldResolver;
}
var readSelection = (ctx, key, select, input, result) => {
  var {
    store
  } = ctx;
  var isQuery = key === store.rootFields.query;
  var entityKey = result && store.keyOfEntity(result) || key;
  if (process.env.NODE_ENV !== 'production') {
    if (!isQuery && !!ctx.store.rootNames[entityKey]) {
      warn('Invalid root traversal: A selection was being read on `' + entityKey + '` which is an uncached root type.\n' + 'The `' + ctx.store.rootFields.mutation + '` and `' + ctx.store.rootFields.subscription + '` types are special ' + 'Operation Root Types and cannot be read back from the cache.', 25, store.logger);
    }
  }
  var typename = !isQuery ? readRecord(entityKey, '__typename') || result && result.__typename : key;
  if (typeof typename !== 'string') {
    return;
  } else if (result && typename !== result.__typename) {
    process.env.NODE_ENV !== 'production' ? warn('Invalid resolver data: The resolver at `' + entityKey + '` returned an ' + 'invalid typename that could not be reconciled with the cache.', 8, store.logger) : void 0;
    return;
  }
  var iterate = makeSelectionIterator(typename, entityKey, false, undefined, select, ctx);
  var hasFields = false;
  var hasNext = false;
  var hasChanged = currentForeignData;
  var node;
  var hasPartials = ctx.partial;
  var output = makeData(input);
  while ((node = iterate()) !== undefined) {
    // Derive the needed data from our node.
    var fieldName = getName(node);
    var fieldArgs = getFieldArguments(node, ctx.variables);
    var fieldAlias = getFieldAlias(node);
    var directives = getDirectives(node);
    var resolver = getFieldResolver(directives, typename, fieldName, ctx);
    var fieldKey = keyOfField(fieldName, fieldArgs);
    var _key = joinKeys(entityKey, fieldKey);
    var fieldValue = readRecord(entityKey, fieldKey);
    var resultValue = result ? result[fieldName] : undefined;
    if (process.env.NODE_ENV !== 'production' && store.schema && typename) {
      isFieldAvailableOnType(store.schema, typename, fieldName, ctx.store.logger);
    }

    // Add the current alias to the walked path before processing the field's value
    ctx.__internal.path.push(fieldAlias);
    // We temporarily store the data field in here, but undefined
    // means that the value is missing from the cache
    var dataFieldValue = undefined;
    if (fieldName === '__typename') {
      // We directly assign the typename as it's already available
      dataFieldValue = typename;
    } else if (resultValue !== undefined && node.selectionSet === undefined) {
      // The field is a scalar and can be retrieved directly from the result
      dataFieldValue = resultValue;
    } else if (currentOperation === 'read' && resolver) {
      // We have a resolver for this field.
      // Prepare the actual fieldValue, so that the resolver can use it,
      // as to avoid the user having to do `cache.resolve(parent, info.fieldKey)`
      // only to get a scalar value.
      var parent = output;
      if (node.selectionSet === undefined && fieldValue !== undefined) {
        parent = {
          ...output,
          [fieldAlias]: fieldValue,
          [fieldName]: fieldValue
        };
      }

      // We have to update the information in context to reflect the info
      // that the resolver will receive
      updateContext(ctx, parent, typename, entityKey, fieldKey, fieldName);
      dataFieldValue = resolver(parent, fieldArgs || {}, store, ctx);
      if (node.selectionSet) {
        // When it has a selection set we are resolving an entity with a
        // subselection. This can either be a list or an object.
        dataFieldValue = resolveResolverResult(ctx, typename, fieldName, _key, getSelectionSet(node), output[fieldAlias] !== undefined ? output[fieldAlias] : input[fieldAlias], dataFieldValue, ownsData(input));
      }
      if (store.schema && dataFieldValue === null && !isFieldNullable(store.schema, typename, fieldName, ctx.store.logger)) {
        // Special case for when null is not a valid value for the
        // current field
        return undefined;
      }
    } else if (!node.selectionSet) {
      // The field is a scalar but isn't on the result, so it's retrieved from the cache
      dataFieldValue = fieldValue;
    } else if (resultValue !== undefined) {
      // We start walking the nested resolver result here
      dataFieldValue = resolveResolverResult(ctx, typename, fieldName, _key, getSelectionSet(node), output[fieldAlias] !== undefined ? output[fieldAlias] : input[fieldAlias], resultValue, ownsData(input));
    } else {
      // Otherwise we attempt to get the missing field from the cache
      var link = readLink(entityKey, fieldKey);
      if (link !== undefined) {
        dataFieldValue = resolveLink(ctx, link, typename, fieldName, getSelectionSet(node), output[fieldAlias] !== undefined ? output[fieldAlias] : input[fieldAlias], ownsData(input));
      } else if (typeof fieldValue === 'object' && fieldValue !== null) {
        // The entity on the field was invalid but can still be recovered
        dataFieldValue = fieldValue;
      }
    }

    // Now that dataFieldValue has been retrieved it'll be set on data
    // If it's uncached (undefined) but nullable we can continue assembling
    // a partial query result
    if (!deferRef && dataFieldValue === undefined && (directives.optional || optionalRef && !directives.required || !!getFieldError(ctx) || store.schema && isFieldNullable(store.schema, typename, fieldName, ctx.store.logger))) {
      // The field is uncached or has errored, so it'll be set to null and skipped
      ctx.partial = true;
      dataFieldValue = null;
    } else if (dataFieldValue === null && (directives.required || optionalRef === false)) {
      if (ctx.store.logger && process.env.NODE_ENV !== 'production' && currentOperation === 'read') {
        ctx.store.logger('debug', `Got value "null" for required field "${fieldName}"${fieldArgs ? ` with args ${JSON.stringify(fieldArgs)}` : ''} on entity "${entityKey}"`);
      }
      dataFieldValue = undefined;
    } else {
      hasFields = hasFields || fieldName !== '__typename';
    }

    // After processing the field, remove the current alias from the path again
    ctx.__internal.path.pop();
    // Check for any referential changes in the field's value
    hasChanged = hasChanged || dataFieldValue !== input[fieldAlias];
    if (dataFieldValue !== undefined) {
      output[fieldAlias] = dataFieldValue;
    } else if (deferRef) {
      hasNext = true;
    } else {
      if (ctx.store.logger && process.env.NODE_ENV !== 'production' && currentOperation === 'read') {
        ctx.store.logger('debug', `No value for field "${fieldName}"${fieldArgs ? ` with args ${JSON.stringify(fieldArgs)}` : ''} on entity "${entityKey}"`);
      }
      // If the field isn't deferred or partial then we have to abort and also reset
      // the partial field
      ctx.partial = hasPartials;
      return undefined;
    }
  }
  ctx.partial = ctx.partial || hasPartials;
  ctx.hasNext = ctx.hasNext || hasNext;
  return isQuery && ctx.partial && !hasFields ? undefined : hasChanged ? output : input;
};
var resolveResolverResult = (ctx, typename, fieldName, key, select, prevData, result, isOwnedData) => {
  if (Array.isArray(result)) {
    var {
      store
    } = ctx;
    // Check whether values of the list may be null; for resolvers we assume
    // that they can be, since it's user-provided data
    var _isListNullable = store.schema ? isListNullable(store.schema, typename, fieldName, ctx.store.logger) : false;
    var hasPartials = ctx.partial;
    var data = makeData(prevData, true);
    var hasChanged = currentForeignData || !Array.isArray(prevData) || result.length !== prevData.length;
    for (var i = 0, l = result.length; i < l; i++) {
      // Add the current index to the walked path before reading the field's value
      ctx.__internal.path.push(i);
      // Recursively read resolver result
      var childResult = resolveResolverResult(ctx, typename, fieldName, joinKeys(key, `${i}`), select, prevData != null ? prevData[i] : undefined, result[i], isOwnedData);
      // After processing the field, remove the current index from the path
      ctx.__internal.path.pop();
      // Check the result for cache-missed values
      if (childResult === undefined && !_isListNullable) {
        ctx.partial = hasPartials;
        return undefined;
      } else {
        ctx.partial = ctx.partial || childResult === undefined && _isListNullable;
        data[i] = childResult != null ? childResult : null;
        hasChanged = hasChanged || data[i] !== prevData[i];
      }
    }
    return hasChanged ? data : prevData;
  } else if (result === null || result === undefined) {
    return result;
  } else if (isOwnedData && prevData === null) {
    return null;
  } else if (isDataOrKey(result)) {
    var _data = prevData || makeData(prevData);
    return typeof result === 'string' ? readSelection(ctx, result, select, _data) : readSelection(ctx, key, select, _data, result);
  } else {
    process.env.NODE_ENV !== 'production' ? warn('Invalid resolver value: The field at `' + key + '` is a scalar (number, boolean, etc)' + ', but the GraphQL query expects a selection set for this field.', 9, ctx.store.logger) : void 0;
    return undefined;
  }
};
var resolveLink = (ctx, link, typename, fieldName, select, prevData, isOwnedData) => {
  if (Array.isArray(link)) {
    var {
      store
    } = ctx;
    var _isListNullable = store.schema ? isListNullable(store.schema, typename, fieldName, ctx.store.logger) : false;
    var newLink = makeData(prevData, true);
    var hasPartials = ctx.partial;
    var hasChanged = currentForeignData || !Array.isArray(prevData) || link.length !== prevData.length;
    for (var i = 0, l = link.length; i < l; i++) {
      // Add the current index to the walked path before reading the field's value
      ctx.__internal.path.push(i);
      // Recursively read the link
      var childLink = resolveLink(ctx, link[i], typename, fieldName, select, prevData != null ? prevData[i] : undefined, isOwnedData);
      // After processing the field, remove the current index from the path
      ctx.__internal.path.pop();
      // Check the result for cache-missed values
      if (childLink === undefined && !_isListNullable) {
        ctx.partial = hasPartials;
        return undefined;
      } else {
        ctx.partial = ctx.partial || childLink === undefined && _isListNullable;
        newLink[i] = childLink || null;
        hasChanged = hasChanged || newLink[i] !== prevData[i];
      }
    }
    return hasChanged ? newLink : prevData;
  } else if (link === null || prevData === null && isOwnedData) {
    return null;
  }
  return readSelection(ctx, link, select, prevData || makeData(prevData));
};
var isDataOrKey = x => typeof x === 'string' || typeof x === 'object' && typeof x.__typename === 'string';

var _write = (store, request, data, error) => {
  if (process.env.NODE_ENV !== 'production') {
    getCurrentDependencies();
  }
  var query = core.formatDocument(request.query);
  var operation = getMainOperation(query);
  var result = {
    data: data || makeData(),
    dependencies: currentDependencies
  };
  var kind = store.rootFields[operation.operation];
  var ctx = makeContext(store, normalizeVariables(operation, request.variables), getFragments(query), kind, kind, error);
  if (process.env.NODE_ENV !== 'production') {
    pushDebugNode(kind, operation);
  }
  writeSelection(ctx, kind, getSelectionSet(operation), result.data);
  if (process.env.NODE_ENV !== 'production') {
    popDebugNode();
  }
  return result;
};
var _writeFragment = (store, query, data, variables, fragmentName) => {
  var fragments = getFragments(query);
  var fragment;
  if (fragmentName) {
    fragment = fragments[fragmentName];
    if (!fragment) {
      process.env.NODE_ENV !== 'production' ? warn('writeFragment(...) was called with a fragment name that does not exist.\n' + 'You provided ' + fragmentName + ' but could only find ' + Object.keys(fragments).join(', ') + '.', 11, store.logger) : void 0;
      return null;
    }
  } else {
    var names = Object.keys(fragments);
    fragment = fragments[names[0]];
    if (!fragment) {
      process.env.NODE_ENV !== 'production' ? warn('writeFragment(...) was called with an empty fragment.\n' + 'You have to call it with at least one fragment in your GraphQL document.', 11, store.logger) : void 0;
      return null;
    }
  }
  var typename = getFragmentTypeName(fragment);
  var dataToWrite = {
    __typename: typename,
    ...data
  };
  var entityKey = store.keyOfEntity(dataToWrite);
  if (!entityKey) {
    return process.env.NODE_ENV !== 'production' ? warn("Can't generate a key for writeFragment(...) data.\n" + 'You have to pass an `id` or `_id` field or create a custom `keys` config for `' + typename + '`.', 12, store.logger) : void 0;
  }
  if (process.env.NODE_ENV !== 'production') {
    pushDebugNode(typename, fragment);
  }
  var ctx = makeContext(store, variables || {}, fragments, typename, entityKey, undefined);
  writeSelection(ctx, entityKey, getSelectionSet(fragment), dataToWrite);
  if (process.env.NODE_ENV !== 'production') {
    popDebugNode();
  }
};
var writeSelection = (ctx, entityKey, select, data) => {
  // These fields determine how we write. The `Query` root type is written
  // like a normal entity, hence, we use `rootField` with a default to determine
  // this. All other root names (Subscription & Mutation) are in a different
  // write mode
  var rootField = ctx.store.rootNames[entityKey] || 'query';
  var isRoot = !!ctx.store.rootNames[entityKey];
  var typename = isRoot ? entityKey : data.__typename;
  if (!typename && entityKey && ctx.optimistic) {
    typename = readRecord(entityKey, '__typename');
  }
  if (!typename) {
    process.env.NODE_ENV !== 'production' ? warn("Couldn't find __typename when writing.\n" + "If you're writing to the cache manually have to pass a `__typename` property on each entity in your data.", 14, ctx.store.logger) : void 0;
    return;
  } else if (!isRoot && entityKey) {
    writeRecord(entityKey, '__typename', typename);
    writeType(typename, entityKey);
  }
  var updates = ctx.store.updates[typename];
  var iterate = makeSelectionIterator(typename, entityKey || typename, false, undefined, select, ctx);
  var node;
  while (node = iterate()) {
    var fieldName = getName(node);
    var fieldArgs = getFieldArguments(node, ctx.variables);
    var fieldKey = keyOfField(fieldName, fieldArgs);
    var fieldAlias = getFieldAlias(node);
    var fieldValue = data[ctx.optimistic ? fieldName : fieldAlias];
    if (
    // Skip typename fields and assume they've already been written above
    fieldName === '__typename' ||
    // Fields marked as deferred that aren't defined must be skipped
    // Otherwise, we also ignore undefined values in optimistic updaters
    fieldValue === undefined && (deferRef || ctx.optimistic && rootField === 'query')) {
      continue;
    }
    if (process.env.NODE_ENV !== 'production') {
      if (ctx.store.schema && typename && fieldName !== '__typename') {
        isFieldAvailableOnType(ctx.store.schema, typename, fieldName, ctx.store.logger);
      }
    }

    // Add the current alias to the walked path before processing the field's value
    ctx.__internal.path.push(fieldAlias);

    // Execute optimistic mutation functions on root fields, or execute recursive functions
    // that have been returned on optimistic objects
    var resolver = void 0;
    if (ctx.optimistic && rootField === 'mutation') {
      resolver = ctx.store.optimisticMutations[fieldName];
      if (!resolver) continue;
    } else if (ctx.optimistic && typeof fieldValue === 'function') {
      resolver = fieldValue;
    }

    // Execute the field-level resolver to retrieve its data
    if (resolver) {
      // We have to update the context to reflect up-to-date ResolveInfo
      updateContext(ctx, data, typename, entityKey || typename, fieldKey, fieldName);
      fieldValue = ensureData(resolver(fieldArgs || {}, ctx.store, ctx));
    }
    if (fieldValue === undefined) {
      if (process.env.NODE_ENV !== 'production') {
        if (!entityKey || !hasField(entityKey, fieldKey) || ctx.optimistic && !readRecord(entityKey, '__typename')) {
          var expected = node.selectionSet === undefined ? 'scalar (number, boolean, etc)' : 'selection set';
          process.env.NODE_ENV !== 'production' ? warn('Invalid undefined: The field at `' + fieldKey + '` is `undefined`, but the GraphQL query expects a ' + expected + ' for this field.', 13, ctx.store.logger) : void 0;
        }
      }
      continue; // Skip this field
    }

    if (node.selectionSet) {
      // Process the field and write links for the child entities that have been written
      if (entityKey && rootField === 'query') {
        var key = joinKeys(entityKey, fieldKey);
        var link = writeField(ctx, getSelectionSet(node), ensureData(fieldValue), key, ctx.optimistic ? readLink(entityKey || typename, fieldKey) : undefined);
        writeLink(entityKey || typename, fieldKey, link);
      } else {
        writeField(ctx, getSelectionSet(node), ensureData(fieldValue));
      }
    } else if (entityKey && rootField === 'query') {
      // This is a leaf node, so we're setting the field's value directly
      writeRecord(entityKey || typename, fieldKey, fieldValue !== null || !getFieldError(ctx) ? fieldValue : undefined);
    }

    // We run side-effect updates after the default, normalized updates
    // so that the data is already available in-store if necessary
    var updater = updates && updates[fieldName];
    if (updater) {
      // We have to update the context to reflect up-to-date ResolveInfo
      updateContext(ctx, data, typename, entityKey || typename, fieldKey, fieldName);
      data[fieldName] = fieldValue;
      updater(data, fieldArgs || {}, ctx.store, ctx);
    }

    // After processing the field, remove the current alias from the path again
    ctx.__internal.path.pop();
  }
};

// A pattern to match typenames of types that are likely never keyable
var KEYLESS_TYPE_RE = /^__|PageInfo|(Connection|Edge)$/;
var writeField = (ctx, select, data, parentFieldKey, prevLink) => {
  if (Array.isArray(data)) {
    var newData = new Array(data.length);
    for (var i = 0, l = data.length; i < l; i++) {
      // Add the current index to the walked path before processing the link
      ctx.__internal.path.push(i);
      // Append the current index to the parentFieldKey fallback
      var indexKey = parentFieldKey ? joinKeys(parentFieldKey, `${i}`) : undefined;
      // Recursively write array data
      var prevIndex = prevLink != null ? prevLink[i] : undefined;
      var links = writeField(ctx, select, data[i], indexKey, prevIndex);
      // Link cannot be expressed as a recursive type
      newData[i] = links;
      // After processing the field, remove the current index from the path
      ctx.__internal.path.pop();
    }
    return newData;
  } else if (data === null) {
    return getFieldError(ctx) ? undefined : null;
  }
  var entityKey = ctx.store.keyOfEntity(data) || (typeof prevLink === 'string' ? prevLink : null);
  var typename = data.__typename;
  if (process.env.NODE_ENV !== 'production') {
    if (parentFieldKey && !ctx.store.keys[data.__typename] && entityKey === null && typeof typename === 'string' && !KEYLESS_TYPE_RE.test(typename)) {
      warn('Invalid key: The GraphQL query at the field at `' + parentFieldKey + '` has a selection set, ' + 'but no key could be generated for the data at this field.\n' + 'You have to request `id` or `_id` fields for all selection sets or create ' + 'a custom `keys` config for `' + typename + '`.\n' + 'Entities without keys will be embedded directly on the parent entity. ' + 'If this is intentional, create a `keys` config for `' + typename + '` that always returns null.', 15, ctx.store.logger);
    }
  }
  var childKey = entityKey || parentFieldKey;
  writeSelection(ctx, childKey, select, data);
  return childKey || null;
};

var invalidateEntity = (entityKey, field, args) => {
  var fields = field ? [{
    fieldKey: keyOfField(field, args)
  }] : inspectFields(entityKey);
  for (var i = 0, l = fields.length; i < l; i++) {
    var {
      fieldKey
    } = fields[i];
    if (readLink(entityKey, fieldKey) !== undefined) {
      writeLink(entityKey, fieldKey, undefined);
    } else {
      writeRecord(entityKey, fieldKey, undefined);
    }
  }
};
var invalidateType = typename => {
  var types = getEntitiesForType(typename);
  for (var entity of types) {
    invalidateEntity(entity);
  }
};

/** Implementation of the {@link Cache} interface as created internally by the {@link cacheExchange}.
 * @internal
 */
class Store {
  constructor(opts) {
    if (!opts) opts = {};
    this.logger = opts.logger;
    this.resolvers = opts.resolvers || {};
    this.directives = opts.directives || {};
    this.optimisticMutations = opts.optimistic || {};
    this.keys = opts.keys || {};
    this.globalIDs = Array.isArray(opts.globalIDs) ? new Set(opts.globalIDs) : !!opts.globalIDs;
    var queryName = 'Query';
    var mutationName = 'Mutation';
    var subscriptionName = 'Subscription';
    if (opts.schema) {
      var schema = buildClientSchema(opts.schema);
      queryName = schema.query || queryName;
      mutationName = schema.mutation || mutationName;
      subscriptionName = schema.subscription || subscriptionName;
      // Only add schema introspector if it has types info
      if (schema.types) this.schema = schema;
    }
    this.updates = opts.updates || {};
    this.rootFields = {
      query: queryName,
      mutation: mutationName,
      subscription: subscriptionName
    };
    this.rootNames = {
      [queryName]: 'query',
      [mutationName]: 'mutation',
      [subscriptionName]: 'subscription'
    };
    this.data = make(queryName);
    if (this.schema && process.env.NODE_ENV !== 'production') {
      expectValidKeyingConfig(this.schema, this.keys, this.logger);
      expectValidUpdatesConfig(this.schema, this.updates, this.logger);
      expectValidResolversConfig(this.schema, this.resolvers, this.logger);
      expectValidOptimisticMutationsConfig(this.schema, this.optimisticMutations, this.logger);
    }
  }
  keyOfField(fieldName, fieldArgs) {
    return keyOfField(fieldName, fieldArgs);
  }
  keyOfEntity(data) {
    // In resolvers and updaters we may have a specific parent
    // object available that can be used to skip to a specific parent
    // key directly without looking at its incomplete properties
    if (contextRef && data === contextRef.parent) {
      return contextRef.parentKey;
    } else if (data == null || typeof data === 'string') {
      return data || null;
    } else if (!data.__typename) {
      return null;
    } else if (this.rootNames[data.__typename]) {
      return data.__typename;
    }
    var key = null;
    if (this.keys[data.__typename]) {
      key = this.keys[data.__typename](data) || null;
    } else if (data.id != null) {
      key = `${data.id}`;
    } else if (data._id != null) {
      key = `${data._id}`;
    }
    var typename = data.__typename;
    var globalID = this.globalIDs === true || this.globalIDs && this.globalIDs.has(typename);
    return globalID || !key ? key : `${typename}:${key}`;
  }
  resolve(entity, field, args) {
    var entityKey = this.keyOfEntity(entity);
    if (entityKey) {
      var fieldKey = keyOfField(field, args);
      var fieldValue = readRecord(entityKey, fieldKey);
      if (fieldValue !== undefined) return fieldValue;
      var fieldLink = readLink(entityKey, fieldKey);
      if (fieldLink !== undefined) fieldLink = ensureLink(this, fieldLink);
      return fieldLink;
    }
  }
  resolveFieldByKey(entity, field, args) {
    return this.resolve(entity, field, args);
  }
  invalidate(entity, field, args) {
    var entityKey = this.keyOfEntity(entity);
    var shouldInvalidateType = entity && typeof entity === 'string' && !field && !args && !this.resolve(entity, '__typename');
    if (shouldInvalidateType) {
      invalidateType(entity);
    } else {
      invariant(entityKey, process.env.NODE_ENV !== "production" ? "Can't generate a key for invalidate(...).\n" + 'You have to pass an id or _id field or create a custom `keys` field for `' + (typeof entity === 'object' ? entity.__typename : entity + '`.') : "", 19);
      invalidateEntity(entityKey, field, args);
    }
  }
  inspectFields(entity) {
    var entityKey = this.keyOfEntity(entity);
    return entityKey ? inspectFields(entityKey) : [];
  }
  updateQuery(input, updater) {
    var request = core.createRequest(input.query, input.variables);
    var output = updater(this.readQuery(request));
    if (output !== null) {
      _write(this, request, output, undefined);
    }
  }
  readQuery(input) {
    var request = core.createRequest(input.query, input.variables);
    return _query(this, request, undefined, undefined).data;
  }
  readFragment(fragment, entity, variables, fragmentName) {
    return _queryFragment(this, core.formatDocument(fragment), entity, variables, fragmentName);
  }
  writeFragment(fragment, data, variables, fragmentName) {
    _writeFragment(this, core.formatDocument(fragment), data, variables, fragmentName);
  }
  link(entity, field, ...rest) {
    var args = rest.length === 2 ? rest[0] : null;
    var link = rest.length === 2 ? rest[1] : rest[0];
    var entityKey = this.keyOfEntity(entity);
    if (entityKey) {
      writeLink(entityKey, keyOfField(field, args), ensureLink(this, link));
    }
  }
}

// Returns the given operation result with added cacheOutcome meta field
var addMetadata = (operation, meta) => core.makeOperation(operation.kind, operation, {
  ...operation.context,
  meta: {
    ...operation.context.meta,
    ...meta
  }
});

// Copy an operation and change the requestPolicy to skip the cache
var toRequestPolicy = (operation, requestPolicy) => {
  return core.makeOperation(operation.kind, operation, {
    ...operation.context,
    requestPolicy
  });
};

/** Exchange factory that creates a normalized cache exchange.
 *
 * @param opts - A {@link CacheExchangeOpts} configuration object.
 * @returns the created normalized cache {@link Exchange}.
 *
 * @remarks
 * Graphcache is a normalized cache, enabled by using the `cacheExchange`
 * in place of `@urql/core`’s. A normalized GraphQL cache uses typenames
 * and key fields in the result to share a single copy for each unique
 * entity across all queries.
 *
 * The `cacheExchange` may be passed a {@link CacheExchangeOpts} object
 * to define custom resolvers, custom updates for mutations,
 * optimistic updates, or to add custom key fields per type.
 *
 * @see {@link https://urql.dev/goto/docs/graphcache} for the full Graphcache docs.
 */
var cacheExchange = opts => ({
  forward,
  client,
  dispatchDebug
}) => {
  var store = new Store(opts);
  if (opts && opts.storage) {
    store.data.hydrating = true;
    opts.storage.readData().then(entries => {
      hydrateData(store.data, opts.storage, entries);
      if (opts.storage.onCacheHydrated) opts.storage.onCacheHydrated();
    });
  }
  var optimisticKeysToDependencies = new Map();
  var mutationResultBuffer = [];
  var operations = new Map();
  var results = new Map();
  var blockedDependencies = new Set();
  var requestedRefetch = new Set();
  var deps = new Map();
  var reexecutingOperations = new Set();
  var dependentOperations = new Set();
  var isBlockedByOptimisticUpdate = dependencies => {
    for (var dep of dependencies.values()) if (blockedDependencies.has(dep)) return true;
    return false;
  };
  var collectPendingOperations = (pendingOperations, dependencies) => {
    if (dependencies) {
      // Collect operations that will be updated due to cache changes
      for (var dep of dependencies.values()) {
        var keys = deps.get(dep);
        if (keys) for (var key of keys.values()) pendingOperations.add(key);
      }
    }
  };
  var executePendingOperations = (operation, pendingOperations, isOptimistic) => {
    // Reexecute collected operations and delete them from the mapping
    for (var key of pendingOperations.values()) {
      if (key !== operation.key) {
        var op = operations.get(key);
        if (op) {
          // Collect all dependent operations if the reexecuting operation is a query
          if (operation.kind === 'query') dependentOperations.add(key);
          var policy = 'cache-first';
          if (requestedRefetch.has(key)) {
            requestedRefetch.delete(key);
            policy = 'cache-and-network';
          }
          client.reexecuteOperation(toRequestPolicy(op, policy));
        }
      }
    }
    if (!isOptimistic) {
      // Upon completion, all dependent operations become reexecuting operations, preventing
      // them from reexecuting prior operations again, causing infinite loops
      var _reexecutingOperations = reexecutingOperations;
      if (operation.kind === 'query') {
        (reexecutingOperations = dependentOperations).add(operation.key);
      }
      (dependentOperations = _reexecutingOperations).clear();
    }
  };

  // This registers queries with the data layer to ensure commutativity
  var prepareForwardedOperation = operation => {
    var optimistic = false;
    if (operation.kind === 'query') {
      // Pre-reserve the position of the result layer
      reserveLayer(store.data, operation.key);
      operations.set(operation.key, operation);
    } else if (operation.kind === 'teardown') {
      // Delete reference to operation if any exists to release it
      operations.delete(operation.key);
      results.delete(operation.key);
      reexecutingOperations.delete(operation.key);
      // Mark operation layer as done
      noopDataState(store.data, operation.key);
      return operation;
    } else if (operation.kind === 'mutation' && operation.context.requestPolicy !== 'network-only') {
      operations.set(operation.key, operation);
      // This executes an optimistic update for mutations and registers it if necessary
      initDataState('write', store.data, operation.key, true, false);
      var {
        dependencies
      } = _write(store, operation, undefined, undefined);
      clearDataState();
      if (dependencies.size) {
        // Update blocked optimistic dependencies
        for (var dep of dependencies.values()) blockedDependencies.add(dep);
        // Store optimistic dependencies for update
        optimisticKeysToDependencies.set(operation.key, dependencies);
        // Update related queries
        var pendingOperations = new Set();
        collectPendingOperations(pendingOperations, dependencies);
        executePendingOperations(operation, pendingOperations, true);
        // Mark operation as optimistic
        optimistic = true;
      }
    }
    return core.makeOperation(operation.kind, {
      key: operation.key,
      query: core.formatDocument(operation.query),
      variables: operation.variables ? filterVariables(getMainOperation(operation.query), operation.variables) : operation.variables
    }, {
      ...operation.context,
      optimistic
    });
  };

  // This updates the known dependencies for the passed operation
  var updateDependencies = (op, dependencies) => {
    for (var dep of dependencies.values()) {
      var depOps = deps.get(dep);
      if (!depOps) deps.set(dep, depOps = new Set());
      depOps.add(op.key);
    }
  };

  // Retrieves a query result from cache and adds an `isComplete` hint
  // This hint indicates whether the result is "complete" or not
  var operationResultFromCache = operation => {
    initDataState('read', store.data, undefined, false, false);
    var result = _query(store, operation, results.get(operation.key), undefined);
    clearDataState();
    var cacheOutcome = result.data ? !result.partial && !result.hasNext ? 'hit' : 'partial' : 'miss';
    results.set(operation.key, result.data);
    operations.set(operation.key, operation);
    updateDependencies(operation, result.dependencies);
    return {
      outcome: cacheOutcome,
      operation,
      data: result.data,
      dependencies: result.dependencies,
      hasNext: result.hasNext
    };
  };

  // Take any OperationResult and update the cache with it
  var updateCacheWithResult = (result, pendingOperations) => {
    // Retrieve the original operation to get unfiltered variables
    var operation = operations.get(result.operation.key) || result.operation;
    if (operation.kind === 'mutation') {
      // Collect previous dependencies that have been written for optimistic updates
      var dependencies = optimisticKeysToDependencies.get(operation.key);
      collectPendingOperations(pendingOperations, dependencies);
      optimisticKeysToDependencies.delete(operation.key);
    }
    if (operation.kind === 'subscription' || result.hasNext) reserveLayer(store.data, operation.key, true);
    var queryDependencies;
    var data = result.data;
    if (data) {
      // Write the result to cache and collect all dependencies that need to be
      // updated
      initDataState('write', store.data, operation.key, false, false);
      var writeDependencies = _write(store, operation, data, result.error).dependencies;
      clearDataState();
      collectPendingOperations(pendingOperations, writeDependencies);
      var prevData = operation.kind === 'query' ? results.get(operation.key) : null;
      initDataState('read', store.data, operation.key, false, prevData !== data);
      var queryResult = _query(store, operation, prevData || data, result.error);
      clearDataState();
      data = queryResult.data;
      if (operation.kind === 'query') {
        // Collect the query's dependencies for future pending operation updates
        queryDependencies = queryResult.dependencies;
        collectPendingOperations(pendingOperations, queryDependencies);
        results.set(operation.key, data);
      }
    } else {
      noopDataState(store.data, operation.key);
    }

    // Update this operation's dependencies if it's a query
    if (queryDependencies) {
      updateDependencies(result.operation, queryDependencies);
    }
    return {
      operation,
      data,
      error: result.error,
      extensions: result.extensions,
      hasNext: result.hasNext,
      stale: result.stale
    };
  };
  return operations$ => {
    // Filter by operations that are cacheable and attempt to query them from the cache
    var cacheOps$ = wonka.share(wonka.map(operationResultFromCache)(wonka.filter(op => op.kind === 'query' && op.context.requestPolicy !== 'network-only')(operations$)));
    var nonCacheOps$ = wonka.filter(op => op.kind !== 'query' || op.context.requestPolicy === 'network-only')(operations$);

    // Rebound operations that are incomplete, i.e. couldn't be queried just from the cache
    var cacheMissOps$ = wonka.map(res => {
      process.env.NODE_ENV !== 'production' ? dispatchDebug({
        type: 'cacheMiss',
        message: 'The result could not be retrieved from the cache',
        operation: res.operation,
        "source": "cacheExchange"
      }) : undefined;
      return process.env.NODE_ENV !== 'production' ? addMetadata(res.operation, {
        cacheOutcome: 'miss'
      }) : res.operation;
    })(wonka.filter(res => res.outcome === 'miss' && res.operation.context.requestPolicy !== 'cache-only' && !isBlockedByOptimisticUpdate(res.dependencies) && !reexecutingOperations.has(res.operation.key))(cacheOps$));

    // Resolve OperationResults that the cache was able to assemble completely and trigger
    // a network request if the current operation's policy is cache-and-network
    var cacheResult$ = wonka.map(res => {
      var {
        requestPolicy
      } = res.operation.context;

      // We reexecute requests marked as `cache-and-network`, and partial responses,
      // if we wouldn't cause a request loop
      var shouldReexecute = requestPolicy !== 'cache-only' && (res.hasNext || requestPolicy === 'cache-and-network' || requestPolicy === 'cache-first' && res.outcome === 'partial' && !reexecutingOperations.has(res.operation.key));
      // Set stale to true anyway, even if the reexecute will be blocked, if the operation
      // is in progress. We can be reasonably sure of that if a layer has been reserved for it.
      var stale = requestPolicy !== 'cache-only' && (shouldReexecute || res.outcome === 'partial' && reexecutingOperations.has(res.operation.key) && hasLayer(store.data, res.operation.key));
      var result = {
        operation: process.env.NODE_ENV !== 'production' ? addMetadata(res.operation, {
          cacheOutcome: res.outcome
        }) : res.operation,
        data: res.data,
        error: res.error,
        extensions: res.extensions,
        stale: stale && !res.hasNext,
        hasNext: shouldReexecute && res.hasNext
      };
      if (!shouldReexecute) ; else if (!isBlockedByOptimisticUpdate(res.dependencies)) {
        client.reexecuteOperation(toRequestPolicy(operations.get(res.operation.key) || res.operation, 'network-only'));
      } else if (requestPolicy === 'cache-and-network') {
        requestedRefetch.add(res.operation.key);
      }
      process.env.NODE_ENV !== 'production' ? dispatchDebug({
        type: 'cacheHit',
        message: `A requested operation was found and returned from the cache.`,
        operation: res.operation,
        data: {
          value: result
        },
        "source": "cacheExchange"
      }) : undefined;
      return result;
    })(wonka.filter(res => res.outcome !== 'miss' || res.operation.context.requestPolicy === 'cache-only')(cacheOps$));

    // Forward operations that aren't cacheable and rebound operations
    // Also update the cache with any network results
    var result$ = forward(wonka.map(prepareForwardedOperation)(wonka.merge([nonCacheOps$, cacheMissOps$])));

    // Results that can immediately be resolved
    var nonOptimisticResults$ = wonka.map(result => {
      var pendingOperations = new Set();
      // Update the cache with the incoming API result
      var cacheResult = updateCacheWithResult(result, pendingOperations);
      // Execute all dependent queries
      executePendingOperations(result.operation, pendingOperations, false);
      return cacheResult;
    })(wonka.filter(result => !optimisticKeysToDependencies.has(result.operation.key))(result$));

    // Prevent mutations that were previously optimistic from being flushed
    // immediately and instead clear them out slowly
    var optimisticMutationCompletion$ = wonka.mergeMap(result => {
      var length = mutationResultBuffer.push(result);
      if (length < optimisticKeysToDependencies.size) {
        return wonka.empty;
      }
      for (var i = 0; i < mutationResultBuffer.length; i++) {
        reserveLayer(store.data, mutationResultBuffer[i].operation.key);
      }
      blockedDependencies.clear();
      var results = [];
      var pendingOperations = new Set();
      var bufferedResult;
      while (bufferedResult = mutationResultBuffer.shift()) results.push(updateCacheWithResult(bufferedResult, pendingOperations));

      // Execute all dependent queries as a single batch
      executePendingOperations(result.operation, pendingOperations, false);
      return wonka.fromArray(results);
    })(wonka.filter(result => optimisticKeysToDependencies.has(result.operation.key))(result$));
    return wonka.merge([nonOptimisticResults$, optimisticMutationCompletion$, cacheResult$]);
  };
};

var policyLevel = {
  'cache-only': 0,
  'cache-first': 1,
  'network-only': 2,
  'cache-and-network': 3
};

/** Input parameters for the {@link offlineExchange}.
 * @remarks
 * This configuration object extends the {@link CacheExchangeOpts}
 * as the `offlineExchange` extends the regular {@link cacheExchange}.
 */

/** Exchange factory that creates a normalized cache exchange in Offline Support mode.
 *
 * @param opts - A {@link OfflineExchangeOpts} configuration object.
 * @returns the created normalized, offline cache {@link Exchange}.
 *
 * @remarks
 * The `offlineExchange` is a wrapper around the regular {@link cacheExchange}
 * which adds logic via the {@link OfflineExchangeOpts.storage} adapter to
 * recognize when it’s offline, when to retry failed mutations, and how
 * to handle longer periods of being offline.
 *
 * @see {@link https://urql.dev/goto/docs/graphcache/offline} for the full Offline Support docs.
 */
var offlineExchange = opts => input => {
  var {
    storage
  } = opts;
  var isOfflineError = opts.isOfflineError || (error => error && error.networkError && !error.response && (typeof navigator !== 'undefined' && navigator.onLine === false || /request failed|failed to fetch|network\s?error/i.test(error.networkError.message)));
  if (storage && storage.onOnline && storage.readMetadata && storage.writeMetadata) {
    var {
      forward: outerForward,
      client,
      dispatchDebug
    } = input;
    var {
      source: reboundOps$,
      next
    } = wonka.makeSubject();
    var failedQueue = [];
    var hasRehydrated = false;
    var isFlushingQueue = false;
    var updateMetadata = () => {
      if (hasRehydrated) {
        var requests = [];
        for (var i = 0; i < failedQueue.length; i++) {
          var operation = failedQueue[i];
          if (operation.kind === 'mutation') {
            requests.push({
              query: core.stringifyDocument(operation.query),
              variables: operation.variables,
              extensions: operation.extensions
            });
          }
        }
        storage.writeMetadata(requests);
      }
    };
    var filterQueue = key => {
      for (var i = failedQueue.length - 1; i >= 0; i--) if (failedQueue[i].key === key) failedQueue.splice(i, 1);
    };
    var flushQueue = () => {
      if (!isFlushingQueue) {
        var sent = new Set();
        isFlushingQueue = true;
        for (var i = 0; i < failedQueue.length; i++) {
          var operation = failedQueue[i];
          if (operation.kind === 'mutation' || !sent.has(operation.key)) {
            sent.add(operation.key);
            if (operation.kind !== 'subscription') {
              next(core.makeOperation('teardown', operation));
              var overridePolicy = 'cache-first';
              for (var _i = 0; _i < failedQueue.length; _i++) {
                var {
                  requestPolicy
                } = failedQueue[_i].context;
                if (policyLevel[requestPolicy] > policyLevel[overridePolicy]) overridePolicy = requestPolicy;
              }
              next(toRequestPolicy(operation, overridePolicy));
            } else {
              next(toRequestPolicy(operation, 'cache-first'));
            }
          }
        }
        isFlushingQueue = false;
        failedQueue.length = 0;
        updateMetadata();
      }
    };
    var forward = ops$ => {
      return wonka.share(wonka.filter(res => {
        if (hasRehydrated && res.operation.kind === 'mutation' && res.operation.context.optimistic && isOfflineError(res.error, res)) {
          failedQueue.push(res.operation);
          updateMetadata();
          return false;
        }
        return true;
      })(outerForward(ops$)));
    };
    var cacheResults$ = cacheExchange({
      ...opts,
      storage: {
        ...storage,
        readData() {
          var hydrate = storage.readData();
          return {
            async then(onEntries) {
              var mutations = await storage.readMetadata();
              for (var i = 0; mutations && i < mutations.length; i++) {
                failedQueue.push(client.createRequestOperation('mutation', core.createRequest(mutations[i].query, mutations[i].variables), mutations[i].extensions));
              }
              onEntries(await hydrate);
              storage.onOnline(flushQueue);
              hasRehydrated = true;
              flushQueue();
            }
          };
        }
      }
    })({
      client,
      dispatchDebug,
      forward
    });
    return operations$ => {
      var opsAndRebound$ = wonka.merge([reboundOps$, wonka.onPush(operation => {
        if (operation.kind === 'query' && !hasRehydrated) {
          failedQueue.push(operation);
        } else if (operation.kind === 'teardown') {
          filterQueue(operation.key);
        }
      })(operations$)]);
      return wonka.filter(res => {
        if (res.operation.kind === 'query') {
          if (isOfflineError(res.error, res)) {
            next(toRequestPolicy(res.operation, 'cache-only'));
            failedQueue.push(res.operation);
            return false;
          } else if (!hasRehydrated) {
            filterQueue(res.operation.key);
          }
        }
        return true;
      })(cacheResults$(opsAndRebound$));
    };
  }
  return cacheExchange(opts)(input);
};

exports.Store = Store;
exports.cacheExchange = cacheExchange;
exports.offlineExchange = offlineExchange;
//# sourceMappingURL=urql-exchange-graphcache.js.map
